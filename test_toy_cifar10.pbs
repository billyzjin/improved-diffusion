#!/bin/bash -l
#PBS -A Brownian_bandits
#PBS -l select=1:system=polaris:ngpus=1
#PBS -l walltime=0:10:00
#PBS -l filesystems=home:eagle
#PBS -q debug
#PBS -N toy_cifar10

echo "=========================================="
echo "TOY CIFAR-10 COMPREHENSIVE TEST"
echo "=========================================="
echo "Job ID: $PBS_JOBID"
echo "User: $(whoami)"
echo "Host: $(hostname)"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "=========================================="

# Load modules
echo "Loading modules..."
module load cray-python/3.11.7 || { echo "ERROR: Failed to load cray-python/3.11.7"; exit 1; }
module load cuda/12.6 || { echo "ERROR: Failed to load cuda/12.6"; exit 1; }
module load nvidia/24.11 || { echo "ERROR: Failed to load nvidia/24.11"; exit 1; }

# Activate PyTorch environment
echo "Activating PyTorch environment..."
source ~/pytorch_env/bin/activate || { echo "ERROR: Failed to activate PyTorch environment"; exit 1; }

# Navigate to project directory
cd /home/billyjin/improved-diffusion

# Set environment variables
export OPENAI_LOGDIR=/tmp/toy_cifar10_test
export CUDA_VISIBLE_DEVICES=0
export OPENAI_LOG_FORMAT="stdout,log"

# Create log directory
mkdir -p $OPENAI_LOGDIR

# Always restore originals, even on failure
restore_originals() {
  cp improved_diffusion/dist_util_original.py improved_diffusion/dist_util.py 2>/dev/null || true
  cp improved_diffusion/image_datasets_original.py improved_diffusion/image_datasets.py 2>/dev/null || true
  cp improved_diffusion/train_util_original.py improved_diffusion/train_util.py 2>/dev/null || true
  cp improved_diffusion/resample_original.py improved_diffusion/resample.py 2>/dev/null || true
}
trap restore_originals EXIT

# Verify package is already installed (installed on login node)
echo "Verifying package installation..."
if ! python3 -c "import improved_diffusion; print('Package installed successfully')" 2>&1; then
    echo "ERROR: Package not found! Please install on login node first:"
    echo "  source ~/pytorch_env/bin/activate"
    echo "  pip install -e ."
    exit 1
fi

#---------------------------------------------------------------------------------
# Apply patches to remove MPI dependencies for single-GPU training
#---------------------------------------------------------------------------------

echo "Creating no-MPI patches..."

# Create dist_util_no_mpi.py
cat > improved_diffusion/dist_util_no_mpi.py << 'EOF'
"""
Distributed utilities - modified to work without MPI for single GPU training.
"""
import os
import torch as th

def setup_dist():
    """Setup for single GPU (no distributed training)."""
    if not th.cuda.is_available():
        print("CUDA not available. Using CPU.")
        return
    th.cuda.set_device(0)
    print(f"Using GPU: {th.cuda.get_device_name(0)}")

def dev():
    """Get the device to use for torch.distributed."""
    return th.device("cuda" if th.cuda.is_available() else "cpu")

def get_world_size():
    """Get the number of processes (always 1 for single GPU)."""
    return 1

def get_rank():
    """Get the rank of this process (always 0 for single GPU)."""
    return 0

def get_local_rank():
    """Get the local rank (always 0 for single GPU)."""
    return 0

def is_main_process():
    """Check if this is the main process (always True for single GPU)."""
    return True

def barrier():
    """Synchronization barrier (no-op for single GPU)."""
    pass

def all_gather(tensor):
    """Gather tensors from all processes (no-op for single GPU)."""
    return [tensor]

def all_reduce(tensor, op=None):
    """Reduce tensors across processes (no-op for single GPU)."""
    return tensor

def broadcast(tensor, src=0):
    """Broadcast tensor to all processes (no-op for single GPU)."""
    return tensor

def synchronize():
    """Synchronize (no-op for single GPU)."""
    pass

def load_state_dict(path, map_location="cpu"):
    """Load state dict from file."""
    return th.load(path, map_location=map_location)

def sync_params(params):
    """Sync parameters across processes (no-op for single GPU)."""
    pass
EOF

# Create image_datasets_no_mpi.py
cat > improved_diffusion/image_datasets_no_mpi.py << 'EOF'
"""
Image datasets - modified to work without MPI.
"""
import os
import torch as th
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import numpy as np

class ImageDataset(Dataset):
    def __init__(self, data_dir, image_size, class_cond=False):
        self.data_dir = data_dir
        self.image_size = image_size
        self.class_cond = class_cond
        
        self.image_paths = []
        for root, dirs, files in os.walk(data_dir):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    self.image_paths.append(os.path.join(root, file))
        
        print(f"Found {len(self.image_paths)} images in {data_dir}")
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        path = self.image_paths[idx]
        
        image = Image.open(path).convert('RGB')
        image = image.resize((self.image_size, self.image_size), Image.LANCZOS)
        image = np.array(image).astype(np.float32) / 255.0
        image = (image - 0.5) / 0.5
        image = th.from_numpy(image).permute(2, 0, 1)
        
        if self.class_cond:
            class_name = os.path.basename(path).split('_')[0]
            class_id = hash(class_name) % 1000
            return image, class_id
        else:
            return image, 0

def load_data(data_dir, batch_size, image_size, class_cond=False):
    """Load image dataset."""
    dataset = ImageDataset(data_dir, image_size, class_cond)
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=2,
        pin_memory=True
    )
    # Convert to infinite iterator and fix format
    def infinite_dataloader():
        while True:
            for batch_data in dataloader:
                if class_cond:
                    images, labels = batch_data
                    cond = {"y": labels} if class_cond else {}
                    yield images, cond
                else:
                    images, _ = batch_data
                    cond = {}
                    yield images, cond
    return infinite_dataloader()
EOF

# Create train_util patch
echo "Creating train_util patch..."
sed -e 's/self.global_batch = self.batch_size \* dist.get_world_size()/self.global_batch = self.batch_size \* 1  # Fixed: no distributed training/' \
    -e 's/if th.cuda.is_available():/if False:  # Disable DDP for single GPU/' \
    -e 's/self.use_ddp = True/self.use_ddp = False/' \
    -e 's/if dist.get_world_size() > 1:/if False:  # Disable multi-GPU check/' \
    -e 's/if dist.get_rank() == 0:/if True:  # Always save on single GPU/' \
    -e 's/dist.barrier()/pass  # No barrier needed for single GPU/' \
    -e 's/dist.get_world_size()/1/g' \
    -e 's/dist.get_rank()/0/g' \
    improved_diffusion/train_util.py > improved_diffusion/train_util_patched.py

# Create resample_no_mpi.py (CRITICAL - this was missing!)
cat > improved_diffusion/resample_no_mpi.py << 'EOF'
"""
Resampling utilities - modified to work without MPI for single GPU training.
"""
import torch as th
import numpy as np
from . import dist_util


class UniformSampler:
    """
    Uniform sampling of timesteps.
    """

    def __init__(self, diffusion):
        self.diffusion = diffusion

    def sample(self, batch_size, device):
        ts = np.random.choice(
            self.diffusion.num_timesteps, batch_size, replace=True
        ).astype(np.int64)
        return th.from_numpy(ts).to(device), th.ones_like(ts, dtype=th.float32)


class LossAwareSampler:
    """
    A wrapper around a sampler that performs loss-aware sampling.
    """

    def __init__(self, diffusion):
        self.diffusion = diffusion
        self.loss_history = np.zeros([diffusion.num_timesteps])

    def weights(self):
        """
        Get sampling weights for each timestep.
        """
        if not self.loss_history.any():
            return np.ones_like(self.loss_history)
        weights = np.sqrt(np.mean(self.loss_history**2, axis=-1))
        weights /= np.sum(weights)
        weights *= 1 - np.exp(-self.loss_history / self.loss_history.mean())
        return weights

    def sample(self, batch_size, device):
        """
        Sample timesteps based on loss history.
        """
        weights = self.weights()
        ts = np.random.choice(
            self.diffusion.num_timesteps, batch_size, replace=True, p=weights
        ).astype(np.int64)
        return th.from_numpy(ts).to(device), th.ones_like(ts, dtype=th.float32)

    def update_with_local_losses(self, local_ts, local_losses):
        """
        Update loss history with local losses (no MPI needed for single GPU).
        """
        for t, loss in zip(local_ts.cpu().numpy(), local_losses.cpu().numpy()):
            if self.loss_history[t] == 0:
                self.loss_history[t] = loss
            else:
                self.loss_history[t] = 0.9 * self.loss_history[t] + 0.1 * loss

    def update_with_all_losses(self, ts, losses):
        """
        Update loss history with all losses.
        """
        for t, loss in zip(ts, losses):
            if self.loss_history[t] == 0:
                self.loss_history[t] = loss
            else:
                self.loss_history[t] = 0.9 * self.loss_history[t] + 0.1 * loss


def create_named_schedule_sampler(name, diffusion):
    """
    Create a named schedule sampler.
    """
    if name == "uniform":
        return UniformSampler(diffusion)
    elif name == "loss-second-moment":
        return LossAwareSampler(diffusion)
    else:
        raise ValueError(f"Unknown schedule sampler: {name}")
EOF

# Apply patches
echo "Applying patches..."
cp improved_diffusion/dist_util.py improved_diffusion/dist_util_original.py
cp improved_diffusion/dist_util_no_mpi.py improved_diffusion/dist_util.py

cp improved_diffusion/image_datasets.py improved_diffusion/image_datasets_original.py
cp improved_diffusion/image_datasets_no_mpi.py improved_diffusion/image_datasets.py

cp improved_diffusion/train_util.py improved_diffusion/train_util_original.py
cp improved_diffusion/train_util_patched.py improved_diffusion/train_util.py

cp improved_diffusion/resample.py improved_diffusion/resample_original.py
cp improved_diffusion/resample_no_mpi.py improved_diffusion/resample.py

#---------------------------------------------------------------------------------
# Prepare dataset
#---------------------------------------------------------------------------------

echo "Preparing CIFAR-10 dataset..."
if [ ! -d "cifar_train" ]; then
    echo "CIFAR-10 not found, creating dummy dataset..."
    if ! python3 prepare_cifar10_manual.py 2>&1; then
        echo "ERROR: Failed to create dummy CIFAR-10 dataset!"
        exit 1
    fi
else
    echo "CIFAR-10 dataset already exists, skipping creation..."
fi

# Verify dataset exists
if [ ! -d "cifar_train" ]; then
    echo "ERROR: CIFAR-10 dataset preparation failed - cifar_train directory not found!"
    exit 1
fi

echo "CIFAR-10 dataset ready: $(find cifar_train -name "*.png" | wc -l) images found"

#---------------------------------------------------------------------------------
# Test all critical code paths with toy parameters
#---------------------------------------------------------------------------------

echo "=========================================="
echo "TESTING ALL CRITICAL CODE PATHS"
echo "=========================================="

# Toy model parameters (very small and fast)
TOY_MODEL_FLAGS="--image_size 32 --num_channels 32 --num_res_blocks 1 --dropout 0.1"
TOY_DIFFUSION_FLAGS="--diffusion_steps 50"
TOY_TRAIN_FLAGS="--lr 1e-3 --batch_size 16 --log_interval 10 --save_interval 20"

# Test 1: Uniform sampler with learn_sigma=False (linear_simple path)
echo "=========================================="
echo "TEST 1: Uniform sampler, learn_sigma=False"
echo "=========================================="
export OPENAI_LOGDIR=/tmp/toy_test1
mkdir -p $OPENAI_LOGDIR

python3 scripts/image_train.py \
    --data_dir ./cifar_train \
    $TOY_MODEL_FLAGS \
    $TOY_DIFFUSION_FLAGS \
    --noise_schedule linear \
    --learn_sigma False \
    $TOY_TRAIN_FLAGS \
    --schedule_sampler uniform \
    --lr_anneal_steps 50

if [ $? -eq 0 ]; then
    echo "✅ TEST 1 PASSED: Uniform sampler, learn_sigma=False"
else
    echo "❌ TEST 1 FAILED: Uniform sampler, learn_sigma=False"
    exit 1
fi

# Test 2: Uniform sampler with learn_sigma=True (linear_hybrid path)
echo "=========================================="
echo "TEST 2: Uniform sampler, learn_sigma=True"
echo "=========================================="
export OPENAI_LOGDIR=/tmp/toy_test2
mkdir -p $OPENAI_LOGDIR

python3 scripts/image_train.py \
    --data_dir ./cifar_train \
    $TOY_MODEL_FLAGS \
    $TOY_DIFFUSION_FLAGS \
    --noise_schedule linear \
    --learn_sigma True \
    --rescale_learned_sigmas False \
    $TOY_TRAIN_FLAGS \
    --schedule_sampler uniform \
    --lr_anneal_steps 50

if [ $? -eq 0 ]; then
    echo "✅ TEST 2 PASSED: Uniform sampler, learn_sigma=True"
else
    echo "❌ TEST 2 FAILED: Uniform sampler, learn_sigma=True"
    exit 1
fi

# Test 3: Loss-aware sampler with learn_sigma=True (cosine_vlb path) - CRITICAL TEST!
echo "=========================================="
echo "TEST 3: Loss-aware sampler, learn_sigma=True (CRITICAL)"
echo "=========================================="
export OPENAI_LOGDIR=/tmp/toy_test3
mkdir -p $OPENAI_LOGDIR

python3 scripts/image_train.py \
    --data_dir ./cifar_train \
    $TOY_MODEL_FLAGS \
    $TOY_DIFFUSION_FLAGS \
    --noise_schedule cosine \
    --learn_sigma True \
    --rescale_learned_sigmas True \
    --use_kl True \
    $TOY_TRAIN_FLAGS \
    --schedule_sampler loss-second-moment \
    --lr_anneal_steps 50

if [ $? -eq 0 ]; then
    echo "✅ TEST 3 PASSED: Loss-aware sampler (CRITICAL - this was the missing test!)"
else
    echo "❌ TEST 3 FAILED: Loss-aware sampler (CRITICAL - this was the missing test!)"
    exit 1
fi

# Test 4: Cosine schedule with uniform sampler
echo "=========================================="
echo "TEST 4: Cosine schedule, uniform sampler"
echo "=========================================="
export OPENAI_LOGDIR=/tmp/toy_test4
mkdir -p $OPENAI_LOGDIR

python3 scripts/image_train.py \
    --data_dir ./cifar_train \
    $TOY_MODEL_FLAGS \
    $TOY_DIFFUSION_FLAGS \
    --noise_schedule cosine \
    --learn_sigma False \
    $TOY_TRAIN_FLAGS \
    --schedule_sampler uniform \
    --lr_anneal_steps 50

if [ $? -eq 0 ]; then
    echo "✅ TEST 4 PASSED: Cosine schedule, uniform sampler"
else
    echo "❌ TEST 4 FAILED: Cosine schedule, uniform sampler"
    exit 1
fi

# Summary
echo "=========================================="
echo "COMPREHENSIVE TEST SUMMARY"
echo "=========================================="
echo "All critical code paths have been tested:"
echo "  ✅ Uniform sampler (used in linear_simple, linear_hybrid, cosine_simple, cosine_hybrid)"
echo "  ✅ Loss-aware sampler (used in cosine_vlb) - CRITICAL MISSING TEST"
echo "  ✅ learn_sigma=False (used in linear_simple, cosine_simple)"
echo "  ✅ learn_sigma=True (used in linear_hybrid, cosine_hybrid, cosine_vlb)"
echo "  ✅ Linear noise schedule"
echo "  ✅ Cosine noise schedule"
echo "  ✅ All MPI patches applied and tested"
echo "=========================================="

echo "=========================================="
echo "COMPREHENSIVE TOY CIFAR-10 TESTS COMPLETED!"
echo "=========================================="
echo "All critical code paths have been tested."
echo "If all tests passed, the full CIFAR-10 experiments should work."
echo "Check the directory for:"
echo "  - Model checkpoints (*.pt files)"
echo "  - Generated samples (samples_*.npz)"
echo "  - Training logs"
echo "=========================================="
